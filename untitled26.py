# -*- coding: utf-8 -*-
"""Untitled26.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTL_Rk6_e34rt5z6JPanJ1xDq1d0TRc-
"""



# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Load Data
data = pd.read_csv('behavioural_dataset.csv')

# Display basic info
print(data.info())
print(data.head())

# Step 1: Data Preprocessing
# Handle missing values using forward fill
data.fillna(method='ffill', inplace=True)

# Visualizing distributions of numeric columns
numeric_cols = data.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(15, 8))
for i, col in enumerate(numeric_cols):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data[col], bins=20, kde=True)
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

# Correlation heatmap
# Select only numeric features for correlation calculation
numeric_data = data.select_dtypes(include=[np.number])

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_data.corr(), annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Heatmap")
plt.show()

# Step 2: Encoding categorical data
le = LabelEncoder()
for col in data.select_dtypes(include='object').columns:
    data[col] = le.fit_transform(data[col])

# Step 3: Scaling the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(data)

# Step 4: PCA for Dimensionality Reduction
pca = PCA(n_components=2)
principal_components = pca.fit_transform(scaled_data)
pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])

# Step 5: Elbow Method to Find Optimal Clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)
    kmeans.fit(pca_df)
    wcss.append(kmeans.inertia_)

plt.figure(figsize=(8, 4))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

# Step 6: Silhouette Score to Evaluate Clustering Quality
for n_cluster in range(2, 11):
    kmeans = KMeans(n_clusters=n_cluster, n_init=10, random_state=42)
    labels = kmeans.fit_predict(pca_df)
    silhouette_avg = silhouette_score(pca_df, labels)
    print(f"For n_clusters = {n_cluster}, the silhouette score is {silhouette_avg:.4f}")

# Step 7: KMeans Clustering (Using Optimal Clusters)
optimal_clusters = 4  # Adjust based on the elbow method result
kmeans = KMeans(n_clusters=optimal_clusters, n_init=10, random_state=42)
data['Cluster'] = kmeans.fit_predict(pca_df)

# Step 8: Plot PCA Clusters with Centroids
plt.figure(figsize=(8, 6))
sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=data['Cluster'], palette='viridis', s=60)
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=150, color='red', label='Centroids')
plt.title('Market Segmentation Using PCA and KMeans')
plt.legend()
plt.show()

# Step 9: Cluster Characteristics Summary
cluster_summary = data.groupby('Cluster').mean()
print("\nCluster Summary:\n", cluster_summary)

# Step 10: Visualize Cluster Characteristics (Heatmap)
plt.figure(figsize=(10, 6))
sns.heatmap(cluster_summary, cmap='YlGnBu', annot=True)
plt.title('Cluster Characteristics Heatmap')
plt.show()

# Step 11: Cluster Size Distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='Cluster', data=data, palette='viridis')
plt.title('Cluster Size Distribution')
plt.xlabel('Cluster')
plt.ylabel('Count')
plt.show()

# Visualizing distributions of numeric columns
numeric_cols = data.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(15, 8))
for i, col in enumerate(numeric_cols):
    plt.subplot(3, 3, i + 1)
    sns.histplot(data[col], bins=20, kde=True)
    plt.title(f"Distribution of {col}")
plt.tight_layout()
plt.show()

# Count Plot: Age vs Total Salary (Binned Salary)
plt.figure(figsize=(12, 6))
sns.countplot(x='Age', hue=pd.cut(data['Total Salary'], bins=5), data=data, palette='viridis')
plt.title('Count Plot: Age vs Total Salary (Binned)')
plt.xlabel('Age')
plt.ylabel('Count')
plt.legend(title='Total Salary (Binned)')
plt.show()

# Count Plot: Age vs No of Dependents
plt.figure(figsize=(12, 6))
sns.countplot(x='Age', hue='No of Dependents', data=data, palette='magma')
plt.title('Count Plot: Age vs No of Dependents')
plt.xlabel('Age')
plt.ylabel('Count')
plt.legend(title='No of Dependents')
plt.show()

# Count Plot: Age vs Price (Binned Price)
plt.figure(figsize=(12, 6))
sns.countplot(x='Age', hue=pd.cut(data['Price'], bins=5), data=data, palette='coolwarm')
plt.title('Count Plot: Age vs Price (Binned)')
plt.xlabel('Age')
plt.ylabel('Count')
plt.legend(title='Price (Binned)')
plt.show()

# PCA Biplot with Loadings (Eigenvectors)
plt.figure(figsize=(10, 8))

# Plot the PCA scores (data points)
sns.scatterplot(x=pca_df['PC1'], y=pca_df['PC2'], hue=data['Cluster'], palette='viridis', s=60)

# Plot the Loadings (Eigenvectors)
loadings = pca.components_.T
for i, (x, y) in enumerate(zip(loadings[:, 0], loadings[:, 1])):
    plt.arrow(0, 0, x, y, color='red', alpha=0.6, head_width=0.05, head_length=0.05)
    plt.text(x * 1.2, y * 1.2, data.columns[i], fontsize=10, color='black')

# Plot the centroids of the clusters
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=150, color='red', label='Centroids')

# Formatting
plt.axhline(0, color='black', linestyle="--", linewidth=0.5)
plt.axvline(0, color='black', linestyle="--", linewidth=0.5)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA Biplot with Clusters and Loadings")
plt.legend()
plt.grid()
plt.show()